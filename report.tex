\documentclass[a4paper,11pt]{article}

\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{fancyheadings}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage[]{mcode}
\usepackage{pdfpages}

 \newcommand{\ba}[1]{\begin{align*}    #1    \end{align*}}
 \newcommand{\ban}[1]{\begin{align}    #1    \end{align}}
 \renewcommand{\vec}[1]{\mathbf{#1}}

 \definecolor{light-gray}{gray}{0.95}

\setlength{\headsep}{1in}

\pagestyle{fancy}

\lhead{ridenour@kth.se \\ herczka@kth.se}
\rhead{Parallel Programming for Large-Scale Problems SF2568}
\title{Parallel Programming for Large-Scale Problems SF2568 \\
Teacher: Michael Hanke \\
Homework 2: The Jacobi Iteration}
\author{Jonathan Ridenour, 780514-7779\\
Mateusz Herczka, 700624-9234}

\begin{document}
\lstdefinestyle{customc}{
 backgroundcolor=\color{light-gray},
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\itshape\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
}

\lstdefinestyle{output}{
 backgroundcolor=\color{light-gray},
 language=bash,
 xleftmargin=\parindent,
 keywordstyle=\color{blue},
 basicstyle=\ttfamily,
 morekeywords={peter@kbpet},
 %alsoletter={:~$},
 morekeywords=[2]{peter@kbpet:},
}
\maketitle
\pagebreak
\section*{Task 1}
\subsection*{a)}
An algorithm for the broadcast operation requiring $\mathcal{O}(\text{log}_2(P))$ communication steps (where $P$ is the number of processes which must receive the information) is as follows.    In each of $n$ steps, let each process $P_i$ ($i$ = 0, 2, . . . , $P-1$) which has the information, send the information to a process $P_{i+P/2^n}$ which needs it.  Each process communicates with that process which is halfway between itself and either the next process which is already informed or the end of the process array.  For example, if $P$ = 8, we get
%\begin{table}[h]
\begin{center}
\begin{tabular}{l r}
 $n=1$: & $P_0 \rightarrow P_4$ \\\\
 $n=2$: & $P_0 \rightarrow P_2$ \\
	 & $P_4 \rightarrow P_6$\\\\
 $n=3$: & $P_0 \rightarrow P_1$ \\
	 & $P_2 \rightarrow P_3$\\
	 & $P_4 \rightarrow P_5$\\
	 & $P_6 \rightarrow P_7$
\end{tabular}
\end{center}
%\end{table}
In this way, the number of active processes doubles with each new step.  Note that if $P = 2^d$ for some integer $d$, then the number of communication steps is exactly $\text{log}_2(P)$, otherwise there will be a number of extra communications which must be carried out.  The number of these "extra" communications will be less than than of another full step however, and thus the dominating term will generally be $\mathcal{O}(\text{log}_2(P))$.

\subsection*{b)}
Assume that $P = 2^n$.  Let $t_s$, $t_d$, and $w$ be the startup time for each processor, the data transfer time, and the size of the information to be transfered, respectively.  Then we can write the time per step as 
\ba{
\text{time per step } = t_s + t_dw.}  
As mentioned above, the number of steps is $n$.  This gives that the total time can be written 
\ba{
\text{total time }=(t_s + t_dw) n = (t_s + t_dw) \cdot \text{log}_2{P}.
}

\subsection*{c)}
The scatter operation can be implemented in a similar fashion as the one-to-all broadcast operation described above.  The root process, which contains all the messages intended for the other processes, sends out half the messages in the first step, one fourth of the messages in the second step, and so on.  Say that we have $P-1$ messages, with $M_i$ intended for process $i$, $i$ = 0, 1, . . . , $P-1$.  Then again, as an example with $P$ = 8, we get the following message schedule.  Note that those messages enclosed in parenthesis to the left of the process indicate those messages which are withheld; while those messages which appear above the right arrow indicate those which are sent to the corresponding process.
\begin{center}
\begin{tabular}{l r}
$n=0$:  & $(M_0,M_1,M_2,M_3,M_4,M_5,M_6,M_7) P_0$ \\\\
 $n=1$: & $(M_0,M_1,M_2,M_3) P_0 \overset{(M_4,M_5,M_6,M_7)}{\longrightarrow} P_4$ \\\\
 $n=2$: & $(M_0,M_1) P_0 \overset{(M_2,M_3)}{ \longrightarrow} P_2$ \\
	 & $(M_4,M_5) P_4 \overset{(M_6,M_7) }{\longrightarrow} P_6$\\\\
 $n=3$: & $(M_0) P_0 \overset{(M_1)}{ \longrightarrow}  P_1$ \\
	 & $(M_2)P_2 \overset{(M_3)}{ \longrightarrow}  P_3$\\
	 & $(M_4)P_4 \overset{(M_5)}{ \longrightarrow}  P_5$\\
	 & $(M_6)P_6 \overset{(M_7)}{ \longrightarrow}    P_7$\\\\
$n=4$: & $(M_0)P_0$\\
& $(M_1)P_1$ \\
& $(M_2)P_2$ \\
& $(M_3)P_3 $\\
& $(M_4)P_4 $\\
& $(M_5)P_5$ \\
& $(M_6)P_6$ \\
& $(M_7)P_7$	 
\end{tabular}
\end{center}  
Note that step $n=0$ indicates the initial state, with all messages assembled at the root process (no communication takes place); whereas step $n=4$ represents the final state, with all messages distributed to the proper process (again, no communication takes place).  Thus the task is carried out with the same number of communication steps as the broadcast algorithm, $\mathcal{O}(\text{log}_2(P))$.
 
\section*{Task 2}
\subsection*{a)}
Let a $[P \times P]$ matrix $A$, a $[P \times 1]$ vector $\vec{x}$, and a $[P \times 1]$ vector $\vec{y}$, be distributed across a $P \times P$ process mesh such that the multiplication $\vec{y} = A \vec{x}$ has just been calculated.  Then component $i$ of $\vec{y}$ is stored on process $i$.  What we want is to transpose $\vec{y}$ so that all components of  $\vec{y}$ will be on all processes.  The algorithm is as follows.  Let the processes in the mesh communicate first "row-wise" then "column-wise."  In the first communication phase, each process on a row in the mesh will send it's value to the others on the row, as well as receive a value from each of them.  Each process sends a cluster of messages intended for the processes in the column above each row process.  In the second phase, processes will communicate with those in the same column, passing along the information they received from the processes in their row.  If we, for example, let $P=2$, then we get the following communication schedule (using the same notation as in Task 1):
\begin{center}
\begin{tabular}{l r}
$n=0$: & $(y_0, y_0, y_0, y_0)P_0$\\
& $(y_1, y_1, y_1, y_1)P_1$ \\
& $(y_2, y_2, y_2, y_2)P_2$ \\
& $(y_3, y_3, y_3, y_3)P_3 $ \\ \\
$n=1$: & $(y_0,y_0) P_0 \overset{(y_0,y_0)}{ \longrightarrow} P_1$ \\
& $(y_1,y_1) P_1 \overset{(y_1,y_1)}{ \longrightarrow} P_0$ \\
& $(y_2,y_2) P_2 \overset{(y_2,y_2)}{ \longrightarrow} P_3$ \\
& $(y_3,y_3) P_3 \overset{(y_3,y_3)}{ \longrightarrow} P_2$ \\ \\
$n=2$: & $(y_0,y_1) P_0 \overset{(y_0,y_1)}{ \longrightarrow} P_2$ \\
& $(y_0,y_1) P_1 \overset{(y_0,y_1)}{ \longrightarrow} P_3$ \\
& $(y_2,y_3) P_2 \overset{(y_2,y_3)}{ \longrightarrow} P_0$ \\
& $(y_2,y_3) P_3 \overset{(y_2,y_3)}{ \longrightarrow} P_1$ \\ \\
$n=3$: & $(y_0, y_1, y_2, y_3)P_0$\\
& $(y_0, y_1, y_2, y_3)P_1$ \\
& $(y_0, y_1, y_2, y_3)P_2$ \\
& $(y_0, y_1, y_2, y_3)P_3 $ 
\end{tabular}
\end{center}  

\subsection*{b)}
In the simple example above, each process sends and receives only one message in each phase.  However in the general case, each process sends and receives one message from each of the other processes in the row (in the case of $2 \times 2$ there is only one other process in the row).  

The size of the clustered messages will be $w \cdot P$ given that the components of $\vec{y}$ are of size $w$. The number of processes per row is of course $P$ and the number of messages sent per process is $P-1$.  This gives the time per phase as
\ba{
\text{time per phase} = (t_s + t_d \cdot wP)(P-1)P.
}
Since the process mesh is two dimensional, there will be two such phases.  This gives the total time as
\ba{
\text{total time} = 2(t_s + t_d \cdot wP)(P-1)P.
}
\section*{Task 3}

We now consider the 1-dimensional differential equation
\ba{
u'' + r(x)u = & \ f(x), \ \ \ \ \ 0<x<1,\\
u(0) = & \  u(1) = 0,
}
where
\ba{
r(x) = & \ x^3,\\
f(x) = & \ \frac{-1}{x^{1.5}}.
}

%\begin{figure}
%\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{brot3}}
%  \caption{Zoom 2.}
%  \label{fig:brot3}
%\end{figure}

\section*{Appendix}
\begin{lstlisting}[style=customc]
\end{lstlisting}
%\begin{thebibliography}{}
%\bibitem{Wilkinson} Barry Wilkinson, Michael Allen {\it Parallel Programming, Techniques and Applications Using Networked Workstations and Parallel Computers}, 2005, Pearson Prentice Hall, pp 9.
%\end{thebibliography}
\end{document}